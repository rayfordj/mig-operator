# 1.0 Prerequisites & Setup

The OpenShift Migration Operator (mig-operator) assists with installation of app migration tooling on OpenShift 3.x and 4.x clusters:

- Migration Controller (mig-controller)
- Migration UI (mig-ui)
- Velero

The above suite of tools is collectively referred to as _CAM_ (cluster app migration) in some parts of this guide.

Before using mig-operator to attempt a migration, verify that the OpenShift clusters you'll be migrating apps between meet version requirements:

| Product         | Versions    |
| -----------     | ----------- |
| OpenShift 3.x   | v3.7+       |
| OpenShift 4.x   | v4.1+       |

Note that temporary object storage will be required perform a migration:
- [AWS S3](https://aws.amazon.com/s3/)
- [NooBaa](https://www.noobaa.io/)
- [Minio](https://min.io/)

## 1.1 Base requirements

* SSH client (for Microsoft Windows users [Putty](https://www.putty.org/) is recommended)
* Recent version of Firefox or Chrome
* oc client [Download from try.openshift.com](http://try.openshift.com)


## 1.1.1 Container Images

For migrations in disconnected environments it may be desired to pull required images ahead of time. 

_Required Images - CAM Tool_
```
quay.io/ocpmigrate/mig-controller:htb3
quay.io/ocpmigrate/mig-operator:htb3
quay.io/ocpmigrate/mig-ui:htb3
quay.io/ocpmigrate/velero:htb3
quay.io/ocpmigrate/migration-plugin:htb3
quay.io/ocpmigrate/velero-restic-restore-helper:htb3
registry.access.redhat.com/rhel7
docker.io/registry:2
```

_Required Images - NooBaa_
```
docker.io/noobaa/noobaa-core:5
docker.io/noobaa/noobaa-core:5
docker.io/noobaa/noobaa-operator:1.1.0
docker.io/centos/mongodb-36-centos7
```



## 1.2 Setup with mig-operator

On both the _source_ and the _destination_ cluster, we'll use mig-operator to install components needed to perform a migration. 

A manifest for deploying mig-operator to OpenShift 3.x and 4.x has been made available as [operator.yml](https://raw.githubusercontent.com/fusor/mig-operator/htb3/operator.yml)


```
# The operator manifest should be downloaded and edited to the specifications above.
# https://raw.githubusercontent.com/fusor/mig-operator/htb3/operator.yml

--------------------------------------------------------------------------------------

# Login to your source cluster (apps will be migrated FROM here)
oc login https://my-source-cluster:8443

# Create mig-operator (requires cluster-admin privilege)
oc create -f operator.yml

--------------------------------------------------------------------------------------

# Login to your destination cluster (apps will be migrated TO here)
oc login https://my-destination-cluster:8443

# Create mig-operator (requires cluster-admin privilege). 
oc create -f operator.yml
```

Next up, we'll tell mig-operator which components it should install for us by creating a `MigrationController` resource.

## 1.3 Creating a `MigrationController` resource

When you install mig-operator, it introduces a `MigrationController` CRD to your cluster.

The sample [`controller-3.yml`](https://raw.githubusercontent.com/fusor/mig-operator/htb3/controller-3.yml) and [`controller-4.yml`](https://raw.githubusercontent.com/fusor/mig-operator/htb3/controller-4.yml) are provided with recommended defaults for:
- Using OpenShift 3.x as the source cluster (_only_ Velero installed)
- Using OpenShift 4.x as the destination _cluster_ (_all_ components: Velero, mig-controller, and mig-ui installed)

If you wish to run app migrations with a different configuration, it's as simple as changing values within the `spec` of each sample MigrationController yaml definition. Note that it is _critical_ that the [mig-controller](https://github.com/fusor/mig-controller) component should only be running on _one of the two_ clusters involved with a migration.

### OpenShift 3.x 
For each OpenShift 3.x cluster that will be involved in the migration, use controller-3.yml as a starting point.

https://raw.githubusercontent.com/fusor/mig-operator/htb3/controller-3.yml


### OpenShift 4.x
For each OpenShift 4.x cluster that will be involved in the migration, use  controller-4.yml as a starting point

https://raw.githubusercontent.com/fusor/mig-operator/htb3/controller-4.yml

## 1.3.1 Configuring the MigrationController spec

The snippet below shows the contents of controller-3.yml, a starting point for installing app migration components on OpenShift 3.x.

```
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
spec:
  [...]
  # 'snapshot_tag' should be htb3 to follow along with this guide
  snapshot_tag: htb3


  # 'migration_velero' should always be 'true'
  migration_velero: true 

  # 'migration_controller' and 'migration_ui' should be 'true' only on the destination cluster
  migration_controller: false
  migration_ui: false

  [...]

  # To install the controller on OpenShift 3 you will need to configure the API endpoint:
  # mig_ui_cluster_api_endpoint: https://replace-with-openshift-cluster-hostname:8443
```

Sample _source cluster_ MigrationController config:
```
  [...]
  snapshot_tag: htb3

  migration_velero: true 
  migration_controller: false
  migration_ui: false
  [...]
```

Sample _destination cluster_ MigrationController config
```
  [...]
  snapshot_tag: htb3

  migration_velero: true 
  migration_controller: true
  migration_ui: true
  [...]
```

You may have also noticed the commented out option for specifying an API endpoint. 

```
  [...]
  # To install the controller on OpenShift 3 you will need to configure the API endpoint:
  # mig_ui_cluster_api_endpoint: https://replace-with-openshift-cluster-hostname:8443
  [...]
```
This should be set to the API endpoint of the OpenShift cluster where the UI will run.

## 1.3.2 Creating the MigrationController

After making necessary edits to the provided starter MigrationController yaml, login to each cluster and create the MigrationController resource. This will kick off creation of the CAM componenets.

```
# Source Cluster
oc login https://my-source-cluster:8443
oc create -f migrationcontroller-src.yml

# Destination Cluster
oc login https://my-destination-cluster:8443
oc create -f migrationcontroller-dest.yml
```

### 1.3.3 Monitoring the work of mig-operator

Once you have created the MigrationController CR, mig-operator will start working on your request. After a few minutes have passed, the complete results should be visible by looking at the running Pods in the `openshift-migration` namespace.

```
# Resulting Pods if you installed all components (destination cluster)

$ oc get pods -n openshift-migration
NAME                                  READY     STATUS    RESTARTS   AGE
controller-manager-89b4f85bb-gtrb8    1/1       Running   0          8m
migration-operator-69546687dd-8trfq   2/2       Running   0          8m
migration-ui-65cc6c6f9f-vmqf8         1/1       Running   0          8m
restic-mh5lb                          1/1       Running   0          7m
restic-p849t                          1/1       Running   0          7m
velero-84b8d9878b-gklfb               1/1       Running   0          8m
```

```
# Resulting Pods if you installed only Velero (source cluster)

$ oc get pods -n openshift-migration
NAME                                  READY     STATUS    RESTARTS   AGE
migration-operator-69546687dd-8trfq   2/2       Running   0          8m
restic-mh5lb                          1/1       Running   0          7m
restic-p849t                          1/1       Running   0          7m
velero-84b8d9878b-gklfb               1/1       Running   0          8m
```


## 1.4 CORS Configuration

After following the steps above, you should have installed the migration UI on _one of your clusters_.

CAMs UI is served out of the cluster
behind its own route, there are 3 distinct origins at play:

* The UI - (Ex: https://mig-ui-mig.apps.examplecluster.com)
* The OAuth Server - (Ex: https://openshift-authentication-openshift-authentication.apps.examplecluster.com)
* The API Server - (Ex: https://api.examplecluster.com:6443)

When the UI is served to the browser through it's route, the browser recognizes
its origin, and **blocks AJAX requests to alternative origins**. This is called
[Cross-Origin Resource Sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS),
and it's a deliberate browser security measure.

The full description of CORS is linked above, but without configuring the non-UI
origin servers, the requests will be blocked. To enable these requests,
the UI's origin should be whitelisted in a `corsAllowedOrigins` list for each
alternative origin. The servers should recognize this list and inspect the
origin of incoming requests. If the origin matches one of the CORS whitelisted 
origins (the UI), there are a set of headers that are returned in the response
that inform the browser the CORS request is accepted and valid.

Additionally, for the same reasons described above, any requests that the UI
may make to source 3.x clusters will also have to be whitelisted by configuring
the same field in the 3.x cluster master-config.yaml. This causes the 3.x API
servers to accept the CORS requests incoming from the UI that was served out
of its OCP4 cluster's route.

**A new `corsAllowedOrigins` entry will need to be added on each OpenShift cluster involved in a migration for the Migration Web UI to function properly.**

### 1.4.1 Configuring CORS on OpenShift 3.x
In order to enable the UI to talk to an OpenShift 3 cluster (whether local or remote) it is necessary to edit the master-config.yaml and restart the OpenShift master nodes. 

To determine the CORS URL that needs to be added retrieve the route URL after installing the controller.
```
$ oc get -n openshift-migration route/migration -o go-template='{{ .spec.host }}{{ println }}'
```

SSH into the OpenShift master nodes, noting the hostname from above

Add the hostname to `/etc/origin/master/master-config.yaml` under corsAllowedOrigins:
```
corsAllowedOrigins:
- //$output-from-previous-command
```

After making these changes on 3.x you'll need to restart OpenShift components to pick up the changed config values. The process for restarting 3.x control plane components [differs based on the OpenShift version](https://docs.openshift.com/container-platform/3.10/architecture/infrastructure_components/kubernetes_infrastructure.html#control-plane-static-pods).

```
# In OpenShift 3.7-3.9, the control plane runs within systemd services
$ systemctl restart atomic-openshift-master-api
$ systemctl restart atomic-openshift-master-controllers


# In OpenShift 3.10-3.11, the control plane runs in 'Static Pods'
$ /usr/local/bin/master-restart api
$ /usr/local/bin/master-restart controller
```

### 1.4.2 Configuring CORS on OpenShift 4.x
On OpenShift 4 cluster resources are modified by the operator if the controller is installed there and you can skip these steps. If you chose not to install the controller on your OpenShift 4 cluster you will need to perform these steps manually.

If you haven't already, determine the CORS URL that needs to be added retrieve the route URL
```
$ oc get -n openshift-migration route/migration -o go-template='{{ .spec.host }}{{ println }}'
```

`oc edit authentication.operator cluster` and ensure the following exist:
```
spec:
  unsupportedConfigOverrides:
    corsAllowedOrigins:
    - //localhost(:|$)
    - //127.0.0.1(:|$)
    - //$output-from-previous-command
```

`$ oc edit kubeapiserver.operator cluster` and ensure the following exist:
```
spec:
  unsupportedConfigOverrides:
    corsAllowedOrigins:
    - //$output-from-previous-command
```

## 1.5 Prepare to use mig-ui in your Browser
To visit the UI, look at the route on the cluster where you specified mig-ui should be installed.
```bash
$ oc get routes migration -n mig -o jsonpath='{.spec.host}'
migration-mig.apps.samplecluster.com
```

Looking at the sample output above, we'd visit the URL below from our browser:
  * https://migration-mig.apps.samplecluster.com

### 1.5.1 Accept Certificates on Source and Destination Clusters

1. Before you can login you will need to accept the certificates with your
   browser on both the source and destination cluster. To do this:
  * Visit the link displayed by the webui for `.well-known/oauth-authorization-server`.
    * For example:
      * OCP 4.1: https://api.samplecluster.com:6443/.well-known/oauth-authorization-server
      * OCP 3.11: https://master1.samplecluster.com/.well-known/oauth-authorization-server
      * OCP 3.11: https://master1.samplecluster.com/api/v1/namespaces
  * Refresh the page
  * Get redirected to login page
2. Login with your credentials for the cluster.

## 1.6 Object Storage Setup

CAM components use S3 object storage as temporary scratch space when performing migrations.  This storage can be any object storage that presents an `S3 like` interface.  Currently, we have tested AWS S3, Noobaa, and Minio.  

### 1.6.1 Object Storage Setup with NooBaa

NooBaa can run on an OpenShift cluster to provide an S3 compatible store for migration scratch space. We recommend loading NooBaa onto the destination cluster.

1. Download the noobaa v1.1.0 CLI from https://github.com/noobaa-operator/releases. 
2. Ensure you have available PVs with capacities of 10 Gi, 50Gi. The NooBaa installer will create PVCs to consume these PVs.
```
# NooBaa PV usage requirements
NAME                          CAPACITY   ACCESS MODES                           
logdir-noobaa-core-0          10Gi       RWO                                    
mongo-datadir-noobaa-core-0   50Gi       RWO,RWX
```

3. Using the `noobaa` CLI tool, install NooBaa to the _destination cluster_. Take note of the output values for 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY' that will be produced for a later step. Also note that an initial bucket has been created called 'first.bucket'
```
$ noobaa install --namespace noobaa

[...]
INFO[0002] System Status:                               
INFO[0003] ✅ Exists: NooBaa "noobaa"                    
INFO[0003] ✅ System Phase is "Ready"                    
[...]
INFO[0003] AWS_ACCESS_KEY_ID: ygeJ5GzAwbBJiSukw8Lv      
INFO[0003] AWS_SECRET_ACCESS_KEY: so2C5X/ttRhiX00DZrOnv0MxV0r5VlOkYmptTU91 
```

4. Expose the NooBaa S3 service to hosts outside the cluster. This is necessary so that both the _source_ and _destination_ cluster will be able to connect to S3 scratch space.
```
$ oc expose svc s3 -n noobaa
```

5. Take note of the NooBaa S3 route URL for a later step.
```
$ oc get route s3 -n noobaa -o jsonpath='http://{.spec.host}'
http://s3-noobaa.apps.destcluster.com
```

6. If you have the [`aws` CLI](https://aws.amazon.com/cli/) installed, you can use it to test that NooBaa is serving the S3 bucket 'first.bucket' from the route obtained in the previous step using the S3 credentials generated during NooBaa install.
```
$ AWS_ACCESS_KEY_ID="ygeJ5GzAwbBJiSukw8Lv" \
AWS_SECRET_ACCESS_KEY="so2C5X/ttRhiX00DZrOnv0MxV0r5VlOkYmptTU91" \
aws s3 ls --endpoint http://s3-noobaa.apps.destcluster.com

2019-09-04 13:21:20 first.bucket
```

### 1.6.2 Object Storage Setup with AWS S3

AWS S3 can serve as migration scratch space as long as both clusters involved in a migration have connectivity to AWS.


#### 1.6.3 Create S3 bucket

```bash
aws s3api create-bucket \
    --bucket <YOUR_BUCKET_NAME> \
    --region us-east-1
```

#### 1.6.4 Create IAM user

1. Create the IAM user:

    ```bash
    aws iam create-user --user-name velero
    ```

2. Attach policies to give `velero` the necessary permissions:

    ```bash
    # Use this policy to grant access to ec2 volumes for PV snapshot purposes
    cat > velero-ec2-snapshot-policy.json <<EOF
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "ec2:DescribeVolumes",
                    "ec2:DescribeSnapshots",
                    "ec2:CreateTags",
                    "ec2:CreateVolume",
                    "ec2:CreateSnapshot",
                    "ec2:DeleteSnapshot"
                ],
                "Resource": "*"
            }
        ]
    }
    EOF
    ```

    ```bash
    # [Option 1] Grant access to a single S3 bucket. Fill in value for `${BUCKET}`.

    BUCKET=<YOUR_BUCKET_NAME>
    cat > velero-s3-policy.json <<EOF
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "s3:GetObject",
                    "s3:DeleteObject",
                    "s3:PutObject",
                    "s3:AbortMultipartUpload",
                    "s3:ListMultipartUploadParts"
                ],
                "Resource": [
                    "arn:aws:s3:::${BUCKET}/*"
                ]
            },
            {
                "Effect": "Allow",
                "Action": [
                    "s3:ListBucket"
                ],
                "Resource": [
                    "arn:aws:s3:::${BUCKET}"
                ]
            }
        ]
    }
    EOF
    ```

    ```bash
    # [Option 2] Grant access to all S3 buckets.

    cat > velero-s3-policy.json <<EOF
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "s3:GetObject",
                    "s3:DeleteObject",
                    "s3:PutObject",
                    "s3:AbortMultipartUpload",
                    "s3:ListMultipartUploadParts"
                ],
                "Resource": [
                    "arn:aws:s3:::*"
                ]
            },
            {
                "Effect": "Allow",
                "Action": [
                    "s3:ListBucket"
                ],
                "Resource": [
                    "arn:aws:s3:::*"
                ]
            }
        ]
    }
    EOF
    ```

    ```bash
    # Attach policy granting IAM access to EC2 EBS
    aws iam put-user-policy \
      --user-name velero \
      --policy-name velero-ebs \
      --policy-document file://velero-ec2-snapshot-policy.json
    ```

    ```bash
    # Attach policy granting IAM access to AWS S3
    aws iam put-user-policy \
      --user-name velero \
      --policy-name velero-s3 \
      --policy-document file://velero-s3-policy.json
    ```


3. Create an access key for the user:

    ```bash
    aws iam create-access-key --user-name velero
    ```

    The result should look like:

    ```json
     {
        "AccessKey": {
              "UserName": "velero",
              "Status": "Active",
              "CreateDate": "2017-07-31T22:24:41.576Z",
              "SecretAccessKey": <AWS_SECRET_ACCESS_KEY>,
              "AccessKeyId": <AWS_ACCESS_KEY_ID>
          }
     }
    ```

---

Next Section: [Section 2 - CAM Overview](./2.md)<br>
[Home](./README.md)
